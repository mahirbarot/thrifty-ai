{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0wCDETaLjhozRD1NI6F/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahirbarot/thrifty-ai/blob/main/task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71mwqgHHeQzB",
        "outputId": "ef3f3c1f-9ae9-4719-b318-0b1a7c752837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: 1.1732865037089109\n",
            "\n",
            "Analytic gradient dW1 (4x3):\n",
            " [[-0.03342232  0.06684463 -0.01671116]\n",
            " [ 0.26428194 -0.52856388  0.13214097]\n",
            " [ 0.         -0.          0.        ]\n",
            " [ 0.07766893 -0.15533787  0.03883447]]\n",
            "\n",
            "Analytic gradient dW2 (1x4):\n",
            " [[-0.15154094 -0.25889318 -0.         -0.17080231]]\n",
            "\n",
            "Max absolute difference between analytic and numerical gradients:\n",
            "W1: 1.5192513913575567e-11\n",
            "W2: 6.006972697036872e-12\n",
            "\n",
            "Updated W1:\n",
            " [[ 0.50005638 -0.14494876  0.64935965]\n",
            " [ 1.49660166 -0.18129699 -0.24735105]\n",
            " [ 1.57921282  0.76743473 -0.46947439]\n",
            " [ 0.53479315 -0.44788391 -0.4696132 ]]\n",
            "\n",
            "Updated W2:\n",
            " [[ 0.25711637 -1.88739093 -1.72491783 -0.5452073 ]]\n",
            "\n",
            "Final loss after one SGD step: 0.9036177230801858\n",
            "Loss decreased after the SGD update.\n"
          ]
        }
      ],
      "source": [
        "# backprop implementation for a 2-layer binary classifier (NumPy only)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "'''  Creating utilities '''\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "# ---------- Forward pass ----------\n",
        "def forward(x, params, y=None):\n",
        "    \"\"\"\n",
        "    x: shape (3,)\n",
        "    params: dict with 'W1' (4x3), 'b1' (4,), 'W2' (1x4), 'b2' (1,)\n",
        "    Returns cache and optionally loss if y provided.\n",
        "    \"\"\"\n",
        "    W1, b1, W2, b2 = params['W1'], params['b1'], params['W2'], params['b2']\n",
        "    # Hidden pre-activation (4,)\n",
        "    z1 = W1.dot(x) + b1\n",
        "    # Hidden activation (4,)\n",
        "    a1 = relu(z1)\n",
        "    # Output pre-activation (scalar inside 1-array)\n",
        "    z2 = W2.dot(a1) + b2  # shape (1,)\n",
        "    # Output activation (pred prob)\n",
        "    a2 = sigmoid(z2)      # shape (1,)\n",
        "    cache = {'x': x, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}\n",
        "    if y is None:\n",
        "        return cache\n",
        "    # Binary cross-entropy for single sample (scalar)\n",
        "    eps = 1e-12\n",
        "    a2_clip = np.clip(a2, eps, 1 - eps)\n",
        "    loss = - (y * np.log(a2_clip) + (1 - y) * np.log(1 - a2_clip)).item()\n",
        "    return cache, loss\n",
        "\n",
        "# backward pass\n",
        "def backward(x, y, params, cache):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns gradients: dW1 (4x3), db1 (4,), dW2 (1x4), db2 (1,)\n",
        "    Key derivatives:\n",
        "    - For sigmoid + BCE output, dL/dz2 = a2 - y\n",
        "    - dW2 = (dL/dz2) * a1^T\n",
        "    - For ReLU hidden, derivative is 1 where z1>0 else 0: dz1 = (W2^T * dz2) * relu'(z1)\n",
        "    \"\"\"\n",
        "\n",
        "    W1, b1, W2, b2 = params['W1'], params['b1'], params['W2'], params['b2']\n",
        "    x = cache['x']\n",
        "    z1, a1, z2, a2 = cache['z1'], cache['a1'], cache['z2'], cache['a2']\n",
        "    # dL/dz2: for sigmoid output with BCE loss, simplifies to (a2 - y)\n",
        "    dz2 = (a2 - y).reshape(1,)  # shape (1,)\n",
        "    # Gradients for W2 and b2\n",
        "    dW2 = dz2.reshape(1,1) * a1.reshape(1, -1)  # shape (1,4)\n",
        "    db2 = dz2.copy()                             # shape (1,)\n",
        "    # Backprop into hidden layer: dL/da1 = W2^T * dz2 (shape (4,))\n",
        "    da1 = W2.T.reshape(-1) * dz2.item()  # shape (4,)\n",
        "    # dz1 = da1 * relu'(z1)\n",
        "    dz1 = da1 * relu_derivative(z1)  # shape (4,)\n",
        "    # Gradients for W1 and b1\n",
        "    dW1 = dz1.reshape(-1,1) * x.reshape(1,-1)  # (4,1) * (1,3) -> (4,3)\n",
        "    db1 = dz1.copy()                            # shape (4,)\n",
        "    grads = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
        "    return grads\n",
        "\n",
        "# sgd update\n",
        "def sgd_update(params, grads, lr=0.1):\n",
        "    params['W1'] -= lr * grads['dW1']\n",
        "    params['b1'] -= lr * grads['db1']\n",
        "    params['W2'] -= lr * grads['dW2']\n",
        "    params['b2'] -= lr * grads['db2']\n",
        "\n",
        "# numerical gradient check\n",
        "def numerical_gradient(params, x, y, eps=1e-5):\n",
        "    \"\"\"\n",
        "    Compute numerical gradients for W1 and W2 by central differences.\n",
        "    Returns num_dW1 (4x3) and num_dW2 (1x4).\n",
        "    \"\"\"\n",
        "    num_dW1 = np.zeros_like(params['W1'])\n",
        "    num_dW2 = np.zeros_like(params['W2'])\n",
        "\n",
        "    # Helper to compute loss given params\n",
        "    def loss_given_params(p):\n",
        "        _, loss = forward(x, p, y)\n",
        "        return loss\n",
        "\n",
        "    # W1\n",
        "    it = np.nditer(params['W1'], flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        orig = params['W1'][idx].copy()\n",
        "        params['W1'][idx] = orig + eps\n",
        "        loss_plus = loss_given_params(params)\n",
        "        params['W1'][idx] = orig - eps\n",
        "        loss_minus = loss_given_params(params)\n",
        "        num_dW1[idx] = (loss_plus - loss_minus) / (2 * eps)\n",
        "        params['W1'][idx] = orig\n",
        "        it.iternext()\n",
        "\n",
        "    # W2\n",
        "    it = np.nditer(params['W2'], flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        orig = params['W2'][idx].copy()\n",
        "        params['W2'][idx] = orig + eps\n",
        "        loss_plus = loss_given_params(params)\n",
        "        params['W2'][idx] = orig - eps\n",
        "        loss_minus = loss_given_params(params)\n",
        "        num_dW2[idx] = (loss_plus - loss_minus) / (2 * eps)\n",
        "        params['W2'][idx] = orig\n",
        "        it.iternext()\n",
        "\n",
        "    return num_dW1, num_dW2\n",
        "\n",
        "# ---------- Script to run everything ----------\n",
        "def run_demo():\n",
        "    rs = np.random.RandomState(42)\n",
        "    # Initialize params\n",
        "    params = {\n",
        "        'W1': rs.randn(4, 3),   # shape (4,3)\n",
        "        'b1': np.zeros(4),\n",
        "        'W2': rs.randn(1, 4),   # shape (1,4)\n",
        "        'b2': np.zeros(1)\n",
        "    }\n",
        "    # Single sample\n",
        "    x = np.array([0.2, -0.4, 0.1])\n",
        "    y = 1\n",
        "    lr = 0.1\n",
        "    eps = 1e-5\n",
        "\n",
        "    # Forward before update\n",
        "    cache, init_loss = forward(x, params, y)\n",
        "    print(\"Initial loss:\", init_loss)\n",
        "\n",
        "    # Backprop (analytic)\n",
        "    grads = backward(x, y, params, cache)\n",
        "    print(\"\\nAnalytic gradient dW1 (4x3):\\n\", grads['dW1'])\n",
        "    print(\"\\nAnalytic gradient dW2 (1x4):\\n\", grads['dW2'])\n",
        "\n",
        "    # Numerical gradient check\n",
        "    num_dW1, num_dW2 = numerical_gradient(params, x, y, eps=eps)\n",
        "    max_diff_W1 = np.max(np.abs(grads['dW1'] - num_dW1))\n",
        "    max_diff_W2 = np.max(np.abs(grads['dW2'] - num_dW2))\n",
        "    print(f\"\\nMax absolute difference between analytic and numerical gradients:\")\n",
        "    print(\"W1:\", max_diff_W1)\n",
        "    print(\"W2:\", max_diff_W2)\n",
        "\n",
        "    # SGD update\n",
        "    sgd_update(params, grads, lr=lr)\n",
        "    print(\"\\nUpdated W1:\\n\", params['W1'])\n",
        "    print(\"\\nUpdated W2:\\n\", params['W2'])\n",
        "\n",
        "    # Forward after update (final loss)\n",
        "    _, final_loss = forward(x, params, y)\n",
        "    print(\"\\nFinal loss after one SGD step:\", final_loss)\n",
        "    if final_loss < init_loss:\n",
        "        print(\"Loss decreased after the SGD update.\")\n",
        "    else:\n",
        "        print(\"Loss did not decrease after the SGD update. (Possible with a single step)\")\n",
        "\n",
        "# Run the demo\n",
        "run_demo()\n"
      ]
    }
  ]
}